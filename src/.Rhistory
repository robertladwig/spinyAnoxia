print(final.boruta)
plot(final.boruta)
boruta.df <- attStats(final.boruta)
boruta_signif =getSelectedAttributes(final.boruta, withTentative = F)
print(boruta.df)
print(boruta_signif)
idx = which( colnames(df.data) %in% boruta_signif)
hyp.data = df.data[,idx]
hyp.data$AF = df.data$AF
hypo1 <- lm(AF ~ ., data = hyp.data)
summary(hypo1)
sum.hypo1 <-summary(hypo1)
step(hypo1)
hypo1 <- lm(AF ~ strat_off + ice_duration + Days.1.mg.L + PO4.P_surf +
PO4.P_bot + Spiny , data = hyp.data)
sum.hypo1 <-summary(hypo1)
AIC(hypo1)
BIC(hypo1)
drop1(hypo1, test = 'F')
relImportance <-calc.relimp(hypo1, type = "lmg", rela = TRUE)
sort(relImportance$lmg, decreasing=TRUE)
boot <- boot.relimp(hypo1, b = 1000, type = c("lmg",
"last", "first", "pratt"), rank = TRUE,
diff = TRUE, rela = TRUE)
booteval.relimp(boot, lev =0.9, nodiff=TRUE) # print result
varImp(hypo1, scale = TRUE)
modeleq1 <- paste0('y = ', round(sum.hypo1$coefficients[1,1],2),
' + ',round(sum.hypo1$coefficients[2,1],2),' Strat.off',
' + ',round(sum.hypo1$coefficients[3,1],2),' Ice.dur',
' + ',round(sum.hypo1$coefficients[4,1],2),' Days.1',
' + ')
modeleq2 <- paste0(round(sum.hypo1$coefficients[5,1],2),' SWF',
' + ',round(sum.hypo1$coefficients[6,1],2),' PO4.surf',
' + ',round(sum.hypo1$coefficients[7,1],2),' PO4.bot',
' + e, where e ~ N(0,',round(sum.hypo1$sigma,2),")")
modeleq1
modeleq2
pred.int <- predict(hypo1, interval = "confidence")
pred.int <- pred.int * attr(sc.info, 'scaled:scale')[1] + attr(sc.info, 'scaled:center')[1]
mydata <- cbind(data.frame('AF'  = df$AF), pred.int)
# PLOT: linear model
p <- ggplot(mydata, aes(fit, AF)) +
stat_smooth(method = lm, col = 'black') +
geom_point(size = 2) +
xlab('Predicted Anoxic Factor [d per season]')+
ylab('Anoxic Factor [d per season]')+
theme_minimal()+
geom_smooth(method = "lm", se=FALSE, color="black", formula = y ~ (x)) +
stat_poly_eq(formula = my.formula,
aes(label = paste( ..rr.label.., sep = "~~~")),
parse = TRUE,size = rel(4.5),
label.y = 0.05,
label.x = 0.1)  +
# annotate("text", x = 61, y = 82.5, label = modeleq1, size = 3)+
# annotate("text", x = 61, y = 80, label = modeleq2, size = 3)+
# annotate("text", x = 60, y = 77.5, label = (paste0('R2 = ',round(sum.hypo1$r.squared,2))), size =5) +
theme(text = element_text(size=10),
axis.text.x = element_text(angle=0, hjust=1));p
my.formula <- y ~ (x)
# PLOT: linear model
p <- ggplot(mydata, aes(fit, AF)) +
stat_smooth(method = lm, col = 'black') +
geom_point(size = 2) +
xlab('Predicted Anoxic Factor [d per season]')+
ylab('Anoxic Factor [d per season]')+
theme_minimal()+
geom_smooth(method = "lm", se=FALSE, color="black", formula = y ~ (x)) +
stat_poly_eq(formula = my.formula,
aes(label = paste( ..rr.label.., sep = "~~~")),
parse = TRUE,size = rel(4.5),
label.y = 0.05,
label.x = 0.1)  +
# annotate("text", x = 61, y = 82.5, label = modeleq1, size = 3)+
# annotate("text", x = 61, y = 80, label = modeleq2, size = 3)+
# annotate("text", x = 60, y = 77.5, label = (paste0('R2 = ',round(sum.hypo1$r.squared,2))), size =5) +
theme(text = element_text(size=10),
axis.text.x = element_text(angle=0, hjust=1));p
p.linear <- p + geom_line(aes(y = lwr), color = "grey", linetype = "dashed")+
geom_line(aes(y = upr), color = "grey", linetype = "dashed"); p.linear
sum.hypo1
booteval.relimp(boot, lev =0.9, nodiff=TRUE) # print result
varImp(hypo1, scale = TRUE)
sort(relImportance$lmg, decreasing=TRUE)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
library(tidyverse)
library(timeSeries)
library(statcomp)
library(zoo)
library(rLakeAnalyzer)
library(lubridate)
library(pracma)
library(broom)
library(ggpmisc)
library(patchwork)
# Package ID: knb-lter-ntl.29.30 Cataloging System:https://pasta.edirepository.org.
# Data set title: North Temperate Lakes LTER: Physical Limnology of Primary Study Lakes 1981 - current.
# Data set creator:  John Magnuson - University of Wisconsin
# Data set creator:  Stephen Carpenter - University of Wisconsin
# Data set creator:  Emily Stanley - University of Wisconsin
# Contact:  NTL Information Manager -  University of Wisconsin  - ntl.infomgr@gmail.com
# Stylesheet v2.11 for metadata conversion into program: John H. Porter, Univ. Virginia, jporter@virginia.edu
inUrl1  <- "https://pasta.lternet.edu/package/data/eml/knb-lter-ntl/29/30/03e232a1b362900e0f059859abe8eb97"
infile1 <- tempfile()
try(download.file(inUrl1,infile1,method="curl"))
if (is.na(file.size(infile1))) download.file(inUrl1,infile1,method="auto")
dt1 <-read.csv(infile1,header=F
,skip=1
,sep=","
,quot='"'
, col.names=c(
"lakeid",
"year4",
"daynum",
"sampledate",
"depth",
"rep",
"sta",
"event",
"wtemp",
"o2",
"o2sat",
"deck",
"light",
"frlight",
"flagdepth",
"flagwtemp",
"flago2",
"flago2sat",
"flagdeck",
"flaglight",
"flagfrlight"    ), check.names=TRUE)
unlink(infile1)
# Fix any interval or ratio columns mistakenly read in as nominal and nominal columns read as numeric or dates read as strings
if (class(dt1$lakeid)!="factor") dt1$lakeid<- as.factor(dt1$lakeid)
if (class(dt1$year4)=="factor") dt1$year4 <-as.numeric(levels(dt1$year4))[as.integer(dt1$year4) ]
if (class(dt1$year4)=="character") dt1$year4 <-as.numeric(dt1$year4)
if (class(dt1$daynum)=="factor") dt1$daynum <-as.numeric(levels(dt1$daynum))[as.integer(dt1$daynum) ]
if (class(dt1$daynum)=="character") dt1$daynum <-as.numeric(dt1$daynum)
# attempting to convert dt1$sampledate dateTime string to R date structure (date or POSIXct)
tmpDateFormat<-"%Y-%m-%d"
tmp1sampledate<-as.Date(dt1$sampledate,format=tmpDateFormat)
# Keep the new dates only if they all converted correctly
if(length(tmp1sampledate) == length(tmp1sampledate[!is.na(tmp1sampledate)])){dt1$sampledate <- tmp1sampledate } else {print("Date conversion failed for dt1$sampledate. Please inspect the data and do the date conversion yourself.")}
rm(tmpDateFormat,tmp1sampledate)
if (class(dt1$depth)=="factor") dt1$depth <-as.numeric(levels(dt1$depth))[as.integer(dt1$depth) ]
if (class(dt1$depth)=="character") dt1$depth <-as.numeric(dt1$depth)
if (class(dt1$rep)!="factor") dt1$rep<- as.factor(dt1$rep)
if (class(dt1$sta)!="factor") dt1$sta<- as.factor(dt1$sta)
if (class(dt1$event)!="factor") dt1$event<- as.factor(dt1$event)
if (class(dt1$wtemp)=="factor") dt1$wtemp <-as.numeric(levels(dt1$wtemp))[as.integer(dt1$wtemp) ]
if (class(dt1$wtemp)=="character") dt1$wtemp <-as.numeric(dt1$wtemp)
if (class(dt1$o2)=="factor") dt1$o2 <-as.numeric(levels(dt1$o2))[as.integer(dt1$o2) ]
if (class(dt1$o2)=="character") dt1$o2 <-as.numeric(dt1$o2)
if (class(dt1$o2sat)=="factor") dt1$o2sat <-as.numeric(levels(dt1$o2sat))[as.integer(dt1$o2sat) ]
if (class(dt1$o2sat)=="character") dt1$o2sat <-as.numeric(dt1$o2sat)
if (class(dt1$deck)=="factor") dt1$deck <-as.numeric(levels(dt1$deck))[as.integer(dt1$deck) ]
if (class(dt1$deck)=="character") dt1$deck <-as.numeric(dt1$deck)
if (class(dt1$light)=="factor") dt1$light <-as.numeric(levels(dt1$light))[as.integer(dt1$light) ]
if (class(dt1$light)=="character") dt1$light <-as.numeric(dt1$light)
if (class(dt1$frlight)!="factor") dt1$frlight<- as.factor(dt1$frlight)
if (class(dt1$flagdepth)!="factor") dt1$flagdepth<- as.factor(dt1$flagdepth)
if (class(dt1$flagwtemp)!="factor") dt1$flagwtemp<- as.factor(dt1$flagwtemp)
if (class(dt1$flago2)!="factor") dt1$flago2<- as.factor(dt1$flago2)
if (class(dt1$flago2sat)!="factor") dt1$flago2sat<- as.factor(dt1$flago2sat)
if (class(dt1$flagdeck)!="factor") dt1$flagdeck<- as.factor(dt1$flagdeck)
if (class(dt1$flaglight)!="factor") dt1$flaglight<- as.factor(dt1$flaglight)
if (class(dt1$flagfrlight)!="factor") dt1$flagfrlight<- as.factor(dt1$flagfrlight)
# Convert Missing Values to NA for non-dates
# Here is the structure of the input data frame:
str(dt1)
attach(dt1)
# The analyses below are basic descriptions of the variables. After testing, they should be replaced.
summary(lakeid)
summary(year4)
summary(daynum)
summary(sampledate)
summary(depth)
summary(rep)
summary(sta)
summary(event)
summary(wtemp)
summary(o2)
summary(o2sat)
summary(deck)
summary(light)
summary(frlight)
summary(flagdepth)
summary(flagwtemp)
summary(flago2)
summary(flago2sat)
summary(flagdeck)
summary(flaglight)
summary(flagfrlight)
# Get more details on character variables
summary(as.factor(dt1$lakeid))
summary(as.factor(dt1$rep))
summary(as.factor(dt1$sta))
summary(as.factor(dt1$event))
summary(as.factor(dt1$frlight))
summary(as.factor(dt1$flagdepth))
summary(as.factor(dt1$flagwtemp))
summary(as.factor(dt1$flago2))
summary(as.factor(dt1$flago2sat))
summary(as.factor(dt1$flagdeck))
summary(as.factor(dt1$flaglight))
summary(as.factor(dt1$flagfrlight))
detach(dt1)
full.do <- readRDS('../data_input/8_Combined_LTER_DO_data_with_full_profiles.rds')
df.livingstone = data.frame('year' = NULL,
'depth' = NULL,
'jz' = NULL,
'alphaz' = NULL,
'id' = NULL)
coeff = data.frame('year' = NULL,
'Jz' = NULL,
'Jv' = NULL,
'Ja' = NULL,
'id' = NULL)
anoxicfactor = data.frame('year' = NULL,
'AF' = NULL,
'id' = NULL)
df.lag = data.frame('year' = NULL,
'timelag' = NULL,
'id' = NULL)
hypso <- read_csv('../data_input/LakeEnsemblR_bathymetry_standard.csv')
H <- hypso$Depth_meter
A <- hypso$Area_meterSquared
df <- dt1 %>%
dplyr::filter(lakeid == 'ME') %>%
dplyr::select(year4, sampledate, depth, wtemp, o2, flago2)
summary(as.factor(df$depth))
if (23 > max(H)){
H <- c(23, H)
A <- c(min(A), A)
}
# calculate the q factor for alpha
if (length(A) <= 2){
areas <- approx(H, A, seq(max(H), min(A),-1))$y
depths <- seq(max(H), min(H),-1)
} else {
areas = A
depths = H
}
fit_q <- function(x, areas, depths){
pred_areas <- max(areas) * (1 - depths/max(depths))^x
fit <- sqrt(sum((areas - pred_areas)^2)/length(areas))
return(fit)
}
# use Brent method to fit q
q <- optim(par = 0.1, fn = fit_q, areas = areas, depths = depths, method = 'Brent', lower = 0.5, upper = 2.0)
# visual check
plot(depths, areas)
lines(depths, max(areas) * (1 - depths/max(depths))^q$par, col = 'red')
q
#### DataRetrieval Tutorial ###
#### Working with USGS gage data from the Yahara River ####
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# Load packages (you may need to install them first)
library(dataRetrieval)
install.packages('dataRetrieval')
#### DataRetrieval Tutorial ###
#### Working with USGS gage data from the Yahara River ####
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
# Load packages (you may need to install them first)
library(dataRetrieval)
library(tidyverse)
library(sf) # pacakge for creating spatial files
library(mapview) # cool package for mapping
library(patchwork) # For adding plots side by side
library(lubridate)
# We're going to be looking at the Yahara River (a river that flows into Lake Mendota)
# USGS 05427718 YAHARA RIVER AT WINDSOR, WI
# https://waterdata.usgs.gov/nwis/uv/?site_no=05427718&PARAmeter_cd=00045
siteNumber <- "05427718"
YaharaInfo <- readNWISsite(siteNumber)
# Use the readNWISsite function to obtain all of the information available for a particular USGS site (or sites) such
# as full station name, drainage area, latitude, and longitude. readNWISsite can also access information about multiple
# sites with a vector input.
siteINFO <- readNWISsite(siteNumber)
# Make the data spatial with the sf (we'll learn about maps next week)
# Just need to tell it which columns have the lat/long data, and the crs (in this case 4326 is WGS 84)
siteINFO.sf <- st_as_sf(siteINFO, coords = c("dec_long_va","dec_lat_va"), remove = FALSE, crs=4326)
# Mapview: make an interactive map quickly:
mapview(siteINFO.sf, layer.name="USGS 05427718 YAHARA RIVER AT WINDSOR, WI")
# Zoom out to see Lake Mendota and Madison
# Use whatNWISdata To discover what data is available for a particular USGS
# site, including measured parameters, period of record, and number of samples
# (count), use the whatNWISdata function.
# It is possible to limit the retrieval
# information to a subset of services. The possible choices for services are:
# "dv" (daily values), "uv", or "iv" (unit values), "qw" (water-quality), "sv"
# (sites visits), "pk" (peak measurements), "gw" (groundwater levels), "ad"
# (sites included in USGS Annual Water Data Reports External Link), "aw" (sites
# monitored by the USGS Active Groundwater Level Network External Link), and
# "id" (historical instantaneous values).
#
# In the following example, we limit the retrieved data to only daily data. The
# default for "service" is all, which returns all of the available data for that
# site. Likewise, there are arguments for parameter code (parameterCd) and
# statistic code (statCd) to filter the results. The default for both is to
# return all possible values (all). The returned count_nu for "uv" data is the
# count of days with returned data, not the actual count of returned values.
# Table 3: Commonly used USGS Stat Codes
# StatCode	shortName
# 00001	Maximum
# 00002	Minimum
# 00003	Mean
# 00008	Median
dailyDataAvailable <- whatNWISdata(siteNumber = siteNumber,
service="dv", statCd="00003")
# What parameter are measured?
dailyDataAvailable$parm_cd
parameterINFO <- readNWISpCode(dailyDataAvailable$parm_cd)
parameterINFO$parameter_nm
# Get daily data
parameterCd <- c("00010","00060")  # Temperature and discharge
statCd <- c("00001","00003")  # Mean and maximum
startDate <- "1995-01-01"
endDate <- "2021-12-31"
yahara.daily <- readNWISdv(siteNumbers = siteNumber,
parameterCd, startDate, endDate, statCd=statCd)
# Make a plot of 2020 mean vs maximum temperature
ggplot(yahara.daily) +
geom_path(aes(x = Date, y = X_00010_00003), col = 'red') +
geom_path(aes(x = Date, y = X_00010_00001), col = 'red4') +
ylab('Temperature (degC)') +
theme_bw()
#### QUESTION (1) ####
# What was the mean annual temperature of the river?
# [Show code]
#### QUESTION (2) ####
# What was the warmest temperature of the river in 2020? What day did this occur on?
# Let's get raw data instead of the daily values
# Raw data (in this case every 15 minutes)
yahara.raw = readNWISuv(siteNumbers = siteNumber, parameterCd=c("00045","00060"), startDate = startDate, endDate = endDate)
#### QUESTION (3) ####
# What is parameter code 00045 measuring? What are the units?
# hint use parameterCdFile %>% filter(parameter_cd == _____)
# Let's create our own mean daily value for discharge
yahara.daily2 = yahara.raw %>% mutate(Date = as.Date(dateTime)) %>% # create date column
group_by(Date) %>%
summarise(meanDischarge = mean(X_00060_00000), sumPrecip = sum(X_00045_00000))
# Let's join with the early daily data to see if there's a difference
yahara.daily2 %>% left_join(yahara.daily) %>% # What is it joining by?
ggplot() +
geom_point(aes(x = X_00060_00003, y = meanDischarge)) +
geom_abline() # this adds a 1:1 line
#### QUESTION (4) ####
# Why might our calculated daily mean be different from the daily values we downloaded that were calculated by the USGS?
# Let's plot mean daily discharge and total daily precipitation (add units to the y-axis labels)
p1 = yahara.daily2 %>% ggplot() +
geom_path(aes(x = Date, y = meanDischarge)) +
ylab('Discharge')
p2 = yahara.daily2 %>% ggplot() +
geom_col(aes(x = Date, y =  sumPrecip)) +
ylab('Precipitation')
p1/p2
df <- yahara.daily2 %>%
mutate(year = year(Date)) %>%
group_by(year) %>%
summarise(discharge = mean(meanDischarge, na.rm = T),
max.discharge = max(meanDischarge, na.rm = T),
min.discharge = min(meanDischarge, na.rm = T),
sum.discharge = sum(meanDischarge, na.rm = T),
precip = mean(sumPrecip, na.rm = T))
ggplot(df, aes(year, max.discharge)) +
geom_line()
write_csv(df, '../data_processed/discharge.csv')
#### QUESTION (5) ####
# Just by observing the two plots, how is precipitation related to discharge?
#### QUESTION (6) ####
# How could you assess this quantitatively? [You don't need to run an analysis,
# rather just tell me how you could assess the relationship between precipitation and discharge?]
# Here we'll show how to use dplyr to aggregate our data to monthly timesteps,
# and then plot that data. One new function we show below is summarize_all which
# means we apply a set of functions (more than one) to all the columns in our
# dataframe. Since we group by month, we are calculating the mean, min, and max
# of temperature, flow, and date. The mutate function allows us to add new
# columns, and so we could easily add different time intervals here and then
# group_by those variables, and summarize by a different time interval.
yahara.month <- yahara.daily %>%
rename(flow_cfs =  X_00060_00003) %>% # let's rename our columns
select(Date, flow_cfs) %>% # select columns the we want (date and flow)
mutate(month = lubridate::month(Date, label=T)) %>%
select(-Date) %>%
group_by(month) %>%
summarize_all(c("mean", "max", "min"), na.rm = T) # take the mean, max, and, min of all cols
yahara.daily
str(yahara.daily)
df
yahara.daily2 %>%
mutate(year = year(Date)) %>%
group_by(year)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
library(tidyverse)
library(timeSeries)
library(statcomp)
library(zoo)
library(rLakeAnalyzer)
library(lubridate)
library(pracma)
library(broom)
library(ggpmisc)
library(patchwork)
library(Boruta)
library(caret)
library(relaimpo)
library(corrplot)
library(RColorBrewer)
physics <- read_csv('../data_processed/physical_timings.csv')
anoxic <- read_csv("../data_processed/anoxicfactor.csv")
fluxes <- read_csv('../data_processed/dosinks.csv')
biomass <- read_csv('../data_processed/3c_biomass_duration.csv')
discharge <- read_csv('../data_processed/discharge.csv')
cw <- readRDS('../data_processed/yearly_clearwater_stats.rds')
nutrients <- read_csv('../data_processed/nutrients.csv')
spiny <- read_csv('../data_processed/spiny.csv')
col.pre <- "steelblue"
col.post <- "orange3"
df.physics <- physics
df.anoxic <- anoxic %>%
dplyr::filter(year != 1995 & year != 2021) %>%
dplyr::select(year, AF)
df.flux <- fluxes %>%
dplyr::filter(year != 1995 & year != 2021) %>%
dplyr::select(year, Jz, Jv, Ja)
df.biomass <- biomass %>%
dplyr::filter(Year != 1995 & Year != 2021) %>%
rename(year = Year)
df.nutrients <- nutrients %>%
dplyr::filter(year != 1995 & year != 2021) %>%
rename(year = year)
df.discharge <- discharge %>%
dplyr::filter(year != 1995 & year != 2021)
df.cw <- cw %>%
dplyr::filter(Year > 1995) %>%
rename(year = Year)
df.spiny <- spiny %>%
dplyr::filter(year != 1995 & year != 2021) %>%
rename(year = year, Spiny = mean)
df <- merge(df.physics, df.anoxic, by = 'year')
df <- merge(df, df.flux, by = 'year')
df <- merge(df, df.biomass, by = 'year')
df <- merge(df, df.discharge, by = 'year')
df <- merge(df, df.cw, by = 'year')
df <- merge(df, df.nutrients, by = 'year')
df <- merge(df, df.spiny, by = 'year')
str(df)
head(df)
df.red <- df[, c("AF",'strat_on' , "strat_off" , "strat_duration" ,
"ice_on" , "ice_off" , "ice_duration" ,
"Jz" , "Jv" , "Ja" ,
"Days.0.5.mg.L" , "Days.1.mg.L" , "Days.1.5.mg.L" ,
"Days.2.mg.L" , "Days.3.mg.L" ,
"sum.discharge" ,# "max.discharge" , "min.discharge" ,
"Clearwater.Duration" , "Prev.Winter.Duration" , "Max.Clearwater.Depth.m" , "Spiny" ,
"pH" , "PO4.P_surf", "PO4.P_bot", "NO3.NO2.N_surf", "NO3.NO2.N_bot", "RSi")]
sc.info <- scale(df.red)
df.data <- as.data.frame(scale(df.red))
boruta_output <- Boruta(AF ~ .,
data = df.data, doTrace=2,
maxRuns = 1e5)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=1.5, las=3, xlab="", main="")  # plot variable importance
final.boruta <- TentativeRoughFix(boruta_output)
print(final.boruta)
plot(final.boruta)
boruta.df <- attStats(final.boruta)
boruta_signif =getSelectedAttributes(final.boruta, withTentative = F)
print(boruta.df)
print(boruta_signif)
idx = which( colnames(df.data) %in% boruta_signif)
hyp.data = df.data[,idx]
hyp.data$AF = df.data$AF
hypo1 <- lm(AF ~ ., data = hyp.data)
summary(hypo1)
sum.hypo1 <-summary(hypo1)
step(hypo1)
df.red <- df[, c("AF",'strat_on' , "strat_off" , "strat_duration" ,
"ice_on" , "ice_off" , "ice_duration" ,
"Jz" , "Jv" , "Ja" ,
"Days.0.5.mg.L" , "Days.1.mg.L" , "Days.1.5.mg.L" ,
"Days.2.mg.L" , "Days.3.mg.L" ,
"sum.discharge" ,# "max.discharge" , "min.discharge" ,
"Clearwater.Duration"  , "Max.Clearwater.Depth.m" , "Spiny" ,
"pH" , "PO4.P_surf", "PO4.P_bot", "NO3.NO2.N_surf", "NO3.NO2.N_bot", "RSi")]
sc.info <- scale(df.red)
df.data <- as.data.frame(scale(df.red))
boruta_output <- Boruta(AF ~ .,
data = df.data, doTrace=2,
maxRuns = 1e5)  # perform Boruta search
boruta_signif <- names(boruta_output$finalDecision[boruta_output$finalDecision %in% c("Confirmed", "Tentative")])  # collect Confirmed and Tentative variables
print(boruta_signif)  # significant variables
plot(boruta_output, cex.axis=1.5, las=3, xlab="", main="")  # plot variable importance
final.boruta <- TentativeRoughFix(boruta_output)
print(final.boruta)
plot(final.boruta)
boruta.df <- attStats(final.boruta)
boruta_signif =getSelectedAttributes(final.boruta, withTentative = F)
print(boruta.df)
print(boruta_signif)
idx = which( colnames(df.data) %in% boruta_signif)
hyp.data = df.data[,idx]
hyp.data$AF = df.data$AF
hypo1 <- lm(AF ~ ., data = hyp.data)
summary(hypo1)
sum.hypo1 <-summary(hypo1)
step(hypo1)
hypo1 <- lm(AF ~ strat_off + ice_duration + Days.1.mg.L + PO4.P_surf +
PO4.P_bot + Spiny , data = hyp.data)
plot(df$ice_duration, df$Prev.Winter.Duration)
